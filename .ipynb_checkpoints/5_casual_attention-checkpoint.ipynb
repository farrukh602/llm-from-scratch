{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2fa5125a-8bc2-4a86-ac70-5e73d61681bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ea967ade-c520-4d51-ba58-1a2ba071d0eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 3])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_emb = torch.tensor(\n",
    "[[0.43, 0.15, 0.89], # Your (x^1)\n",
    "[0.55, 0.87, 0.66], # journey (x^2)\n",
    "[0.57, 0.85, 0.64], # starts (x^3)\n",
    "[0.22, 0.58, 0.33], # with (x^4)\n",
    "[0.77, 0.25, 0.10], # one (x^5)\n",
    "[0.05, 0.80, 0.55]] # step (x^6)\n",
    ")\n",
    "batch = torch.stack((inputs_emb,inputs_emb))\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "24bba44b-bbc4-49d0-8981-026ca2d6b02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super(CasualAttention, self).__init__()\n",
    "        self.d_out=d_out\n",
    "\n",
    "        # Trainable weights\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries@keys.transpose(1,2)\n",
    "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens],-torch.inf )\n",
    "\n",
    "        # keep in note that attn_weights are actualy non trainable weights. the are just scaling the value matrices. \n",
    "        attn_weights  = torch.softmax(attn_scores/keys.shape[0]**0.5, dim=1)\n",
    "        \n",
    "        #Apply an additional dropout mask\n",
    "        # (upper right) to zero out additional attention weights to reduce overfitting during training\n",
    "        attn_weights=self.dropout(attn_weights)\n",
    "        \n",
    "        context_vec = attn_weights@values\n",
    "        return context_vec, attn_weights, self.W_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "04a1e9a0-dffb-45ea-995c-08c1e624ab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(234)\n",
    "context_length = batch.shape[1]\n",
    "d_in = batch.shape[-1]\n",
    "d_out=10\n",
    "ca = CasualAttention(d_in, d_out, context_length,0.0)\n",
    "context_vectors, attn_w, trainable_w= ca(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5a821b63-5ac8-4007-8789-321ff72ece12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0597,  0.0536,  0.0659, -0.0358,  0.0436,  0.0069, -0.0105,\n",
       "           0.0557,  0.0589, -0.0707],\n",
       "         [-0.1557,  0.1229,  0.1643, -0.0980,  0.0727,  0.0099, -0.0601,\n",
       "           0.0760,  0.1096, -0.0888],\n",
       "         [-0.2828,  0.2121,  0.2956, -0.1768,  0.1122,  0.0156, -0.1239,\n",
       "           0.1118,  0.1775, -0.1210],\n",
       "         [-0.5215,  0.4060,  0.5396, -0.3458,  0.2033,  0.0199, -0.2383,\n",
       "           0.1585,  0.3250, -0.1803],\n",
       "         [-0.9754,  0.4949,  1.0260, -0.4512,  0.1846,  0.0889, -0.5248,\n",
       "           0.4315,  0.3808, -0.2649],\n",
       "         [-1.0276,  0.8671,  1.0486, -0.7517,  0.4156,  0.0121, -0.4834,\n",
       "           0.1836,  0.6621, -0.2632]],\n",
       "\n",
       "        [[-0.0597,  0.0536,  0.0659, -0.0358,  0.0436,  0.0069, -0.0105,\n",
       "           0.0557,  0.0589, -0.0707],\n",
       "         [-0.1557,  0.1229,  0.1643, -0.0980,  0.0727,  0.0099, -0.0601,\n",
       "           0.0760,  0.1096, -0.0888],\n",
       "         [-0.2828,  0.2121,  0.2956, -0.1768,  0.1122,  0.0156, -0.1239,\n",
       "           0.1118,  0.1775, -0.1210],\n",
       "         [-0.5215,  0.4060,  0.5396, -0.3458,  0.2033,  0.0199, -0.2383,\n",
       "           0.1585,  0.3250, -0.1803],\n",
       "         [-0.9754,  0.4949,  1.0260, -0.4512,  0.1846,  0.0889, -0.5248,\n",
       "           0.4315,  0.3808, -0.2649],\n",
       "         [-1.0276,  0.8671,  1.0486, -0.7517,  0.4156,  0.0121, -0.4834,\n",
       "           0.1836,  0.6621, -0.2632]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "39cb989a-ac93-4668-acb2-02b633c9e034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1543, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1374, 0.1487, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1403, 0.1524, 0.1794, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1795, 0.2198, 0.2585, 0.3259, 0.0000, 0.0000],\n",
       "         [0.2457, 0.3119, 0.3649, 0.3919, 0.5741, 0.0000],\n",
       "         [0.1428, 0.1672, 0.1972, 0.2822, 0.4259, 1.0000]],\n",
       "\n",
       "        [[0.1543, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1374, 0.1487, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1403, 0.1524, 0.1794, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1795, 0.2198, 0.2585, 0.3259, 0.0000, 0.0000],\n",
       "         [0.2457, 0.3119, 0.3649, 0.3919, 0.5741, 0.0000],\n",
       "         [0.1428, 0.1672, 0.1972, 0.2822, 0.4259, 1.0000]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "164c973a-1615-42f1-8c8e-1244e69c80bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4938,  0.0943, -0.4663],\n",
       "        [-0.5758,  0.0860,  0.2456],\n",
       "        [-0.1564, -0.5161, -0.4219],\n",
       "        [ 0.4813,  0.4558, -0.3578],\n",
       "        [ 0.3532,  0.5202,  0.2513],\n",
       "        [-0.1710,  0.5219,  0.0773],\n",
       "        [ 0.2211, -0.4895, -0.0674],\n",
       "        [ 0.4343, -0.0683, -0.3715],\n",
       "        [ 0.5294, -0.5342, -0.1516],\n",
       "        [-0.0368,  0.4616,  0.5172]], requires_grad=True)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_w.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b348310f-80c5-4867-86a2-c1ab459c765c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
