{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d16fe6fa-c3f8-41fa-a934-d73c8a7ea3a0",
   "metadata": {},
   "source": [
    "### Data sampling with a sliding window\n",
    "The next step before we can finally create the embeddings for the LLM is to generate the input-target pairs required for training an LLM. LLMs\n",
    "are pretrained by predicting the next word in a text.\n",
    "<br>Given a text sample, extract input blocks as subsamples that serve as input to the\n",
    "LLM, and the LLM's prediction task during training is to predict the next word that follows the\n",
    "input block. During training, we mask out all words that are past the target. Note that the text would undergo tokenization before the LLM can process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc894515-0f57-4fda-bc70-82a399f74b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e8e1522-4378-4d2f-81ac-e7b236c89c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5560\n"
     ]
    }
   ],
   "source": [
    "# Tokenize 'The Verdict' Story\n",
    "with open('the_verdict.txt',mode='r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "enc_of_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_of_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "423a53e1-a973-44f9-b9b4-4c2be41cf1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling from tokens\n",
    "enc_sample = enc_of_text[50:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861f6981-a8dd-43b6-8ca3-355afb83f4eb",
   "metadata": {},
   "source": [
    "Create input-target pairs for the next-word prediction.<br> x: input tokens<br> y: torken tokens<br> target tokens are actually input tokens shifted by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "430889c0-f39c-4278-b837-f7399e5a7f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:[7026, 15632, 438, 2016, 257, 922]\n",
      "targets:\t[15632, 438, 2016, 257, 922, 5891]\n"
     ]
    }
   ],
   "source": [
    "context_size = 6\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f'inputs:{x}\\ntargets:\\t{y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b486b13-099b-47f6-be29-2cf06fed009a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7026] -----> 15632\n",
      "[7026, 15632] -----> 438\n",
      "[7026, 15632, 438] -----> 2016\n",
      "[7026, 15632, 438, 2016] -----> 257\n",
      "[7026, 15632, 438, 2016, 257] -----> 922\n",
      "[7026, 15632, 438, 2016, 257, 922] -----> 5891\n"
     ]
    }
   ],
   "source": [
    "# input target pairs by token ids\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, '----->', desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0af41366-85b5-4f4c-8d8a-c0369ebfd3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " cheap ----->  genius\n",
      " cheap genius -----> --\n",
      " cheap genius-- -----> though\n",
      " cheap genius--though ----->  a\n",
      " cheap genius--though a ----->  good\n",
      " cheap genius--though a good ----->  fellow\n"
     ]
    }
   ],
   "source": [
    "# input-target pairs by converting token ids into text \n",
    "for i in range(1,context_size+1):\n",
    "    context = tokenizer.decode(enc_sample[:i])\n",
    "    desired = tokenizer.decode([enc_sample[i]])\n",
    "    print(context, '----->', desired)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e2b6ba-2274-4d74-aaf6-60beb55d19cf",
   "metadata": {},
   "source": [
    "#### Implement a data loader that fetches the input-target pairs from the training dataset using a sliding window approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a36e40e9-b512-4cd7-9d82-c7fe32d677dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6dcf576b-0b64-4375-9366-3240c7edec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length, stride):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids=[]\n",
    "        self.target_ids=[]\n",
    "        token_ids = tokenizer.encode(text)\n",
    "        for i in range(0,len(token_ids)-max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            target_chunk = token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "# creating dataloader function\n",
    "def create_dataloader_v1(text, batch_size=64, max_length=256, stride=128, shuffle=True, drop_last=True):\n",
    "    tokenizer = tiktoken.get_encoding('gpt2')\n",
    "    dataset = GPTDatasetV1(text, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cdf3b7b5-8e6b-493f-b2cd-54ff6d7de901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  464,  4643, 11600,   628],\n",
      "        [  198,   197,   197,   197],\n",
      "        [  197,   197,  7407,   342],\n",
      "        [  854, 41328,   628,   628],\n",
      "        [  198,   198,  1129,  2919],\n",
      "        [  628,   628,   198,   198],\n",
      "        [ 3109,  9213,   422, 11145],\n",
      "        [  271,  1668,   319,  2795]])\n",
      "tensor([[ 4643, 11600,   628,   198],\n",
      "        [  197,   197,   197,   197],\n",
      "        [  197,  7407,   342,   854],\n",
      "        [41328,   628,   628,   198],\n",
      "        [  198,  1129,  2919,   628],\n",
      "        [  628,   198,   198,  3109],\n",
      "        [ 9213,   422, 11145,   271],\n",
      "        [ 1668,   319,  2795,   678]])\n"
     ]
    }
   ],
   "source": [
    "#apply dataloader on raw text\n",
    "dataloader= create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "data_iter=iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841ada6e-b611-4a82-995a-b2bb27bc357e",
   "metadata": {},
   "source": [
    "We have increased the stride to 4. This is to utilize the dataset fully (not skipping a single word) along with avoiding the overlap bw the batches, since more overlap could lead to the increased overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf253df7-9b8e-4255-877d-e0f945c75404",
   "metadata": {},
   "source": [
    "### Creating Token Embeddings\n",
    "Preparing the input text for an LLM involves tokenizing text, converting text tokens\n",
    "to token IDs, and converting token IDs into vector embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9bfce0dd-7d76-46ff-b916-f8e752ca73f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0507, -0.2138, -0.1526],\n",
       "        [ 0.3901,  1.0490, -1.0131],\n",
       "        [-1.1523,  1.8710,  2.1880],\n",
       "        [-0.0932, -1.1347, -0.2361],\n",
       "        [ 1.3525,  0.9610,  0.2923],\n",
       "        [ 0.2219, -0.2735,  1.8279]], requires_grad=True)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "input_ids = torch.tensor([2,3,5,1,4,])\n",
    "\n",
    "vocab_size=6    # Number of embeddings\n",
    "output_dim = 3  # Embedding Dimension\n",
    "\n",
    "torch.manual_seed(2345)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim) # Embedding layer is a weight matrix. categorical data converted into dense numerical data\n",
    "embedding_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2cb61940-36a2-400c-ada1-be8e9df32940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1523,  1.8710,  2.1880],\n",
       "        [-0.0932, -1.1347, -0.2361],\n",
       "        [ 0.2219, -0.2735,  1.8279],\n",
       "        [ 0.3901,  1.0490, -1.0131],\n",
       "        [ 1.3525,  0.9610,  0.2923]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer( torch.tensor([2,3,5,1,4,]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa053aaa-a9e4-4843-82a6-e33dbcefed9b",
   "metadata": {},
   "source": [
    "The embedding layer converts a token ID into the same vector representation\n",
    "regardless of where it is located in the input sequence. For example, the token ID 5, whether it's\n",
    "in the first or third position in the token ID input vector, will result in the same embedding\n",
    "vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8844c9f-60b3-4660-b69a-dd46fff72599",
   "metadata": {},
   "source": [
    "Embedding layers perform a look-up operation, retrieving the embedding vector\n",
    "corresponding to the token ID from the embedding layer's weight matrix. For instance, the\n",
    "embedding vector of the token ID 5 is the sixth row of the embedding layer weight matrix (it is\n",
    "the sixth instead of the fifth row because Python starts counting at 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af07b0c0-1c99-4be1-b4ba-b9ac471c17b5",
   "metadata": {},
   "source": [
    "### absolute positional embeddings\n",
    "In principle, the deterministic, position-independent embedding of the token\n",
    "ID is good for reproducibility purposes. However, since the self-attention\n",
    "mechanism of LLMs itself is also position-agnostic, it is helpful to inject\n",
    "additional position information into the LLM.\n",
    "<br>Absolute positional embeddings are directly associated with specific\n",
    "positions in a sequence. For each position in the input sequence, a unique\n",
    "embedding is added to the token's embedding to convey its exact location.\n",
    "For instance, the first token will have a specific positional embedding, the\n",
    "second token another distinct embedding, and so on.\n",
    "<br>Positional embeddings are added to the token embedding vector to create the input\n",
    "embeddings for an LLM. The positional vectors have the same dimension as the original token\n",
    "embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "74fbad3a-b8af-4044-81f9-48498e95cc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[  618,   673,  2540,   284],\n",
      "        [   12, 11649,    32,  2339],\n",
      "        [ 1359,   319,   262, 34686],\n",
      "        [  292,   611,   339,   550],\n",
      "        [  326,   339,  1239,  1807],\n",
      "        [  339, 13055,    11,   345],\n",
      "        [  739, 10724,   262,  6846],\n",
      "        [  284,   423,   546,    26]])\n",
      "\n",
      "input shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(341)\n",
    "\n",
    "output_dim = 256 # lenght of embedding vector\n",
    "vocab_size=50257 # Number of bpe tokens in tiktoken\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "# let's have a batch size of 8 with 4 tokens each. the resulted embedding matrix would have shape 8x4x256\n",
    "# let's instantiate the dataloader with batch size of 8\n",
    "max_length=4\n",
    "data_loader = create_dataloader_v1(raw_text, batch_size=8,max_length=max_length, stride=max_length)\n",
    "\n",
    "data_iter = iter(data_loader)\n",
    "inputs, targets = next(data_iter)\n",
    "print('Token IDs:\\n',inputs)\n",
    "print()\n",
    "print('input shape:\\n', inputs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a8564d9f-b1cd-4779-bca7-6e7e78b3b44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# let's now use embedding layer to embed these token ids into 256 dimensional vectors\n",
    "token_embedding_vectors = token_embedding_layer(inputs)\n",
    "print(token_embedding_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c35bde-c384-4b0e-9814-4224f7b43bdc",
   "metadata": {},
   "source": [
    "For a GPT model's absolute embedding approach, we just need to create\n",
    "another embedding layer that has the same dimension as the\n",
    "`token_embedding_layer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "292792b8-2c41-4e55-836a-b6f95dde884e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(341)\n",
    "context_length = max_length\n",
    "abs_pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "abs_pos_embedding_vectors=abs_pos_embedding_layer(torch.arange(context_length))\n",
    "abs_pos_embedding_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f2d5da95-3bdb-46c2-9aa1-203732a6c009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(context_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4579d6-d3b2-4824-9a98-dfb588ac209b",
   "metadata": {},
   "source": [
    "Now add these directly to the token embeddings,\n",
    "where PyTorch will add the 4x256-dimensional pos_embeddings tensor to\n",
    "each 4x256-dimensional token embedding tensor in each of the 8 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "27253e99-d2b3-4356-a137-024b07c0bbc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 256])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings = token_embedding_vectors+abs_pos_embedding_vectors\n",
    "input_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b7c975-b6b5-4250-85ac-2330ad40bb49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
