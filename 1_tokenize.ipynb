{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df1ca4eb-26a4-4a79-bd08-21d409712238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c88435b9-5e86-472e-ad0c-2689792eea02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 21937\n"
     ]
    }
   ],
   "source": [
    "with open(\"the_verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total number of character:\", len(raw_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa01e36d-72b7-4b44-a0ed-8eb315703761",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4886"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessed_text = re.sub(r'([,.?_!\"()\\']|--|\\s)',\" \", raw_text).split()\n",
    "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "len(preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2bf916-2a42-437a-b549-22624fd60572",
   "metadata": {},
   "source": [
    "### Converting tokens into token IDS\n",
    "Build vocabulary set be removing duplicate words. Then arrange all the unique words in alphabatical order and assign each word with an integer ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f555a97-3971-4338-aa46-8aca3a1ec051",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1266"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use set() method to have all the words unique\n",
    "all_words= sorted(set(preprocessed))\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "len(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bc1537-0c9d-4c96-8570-8c73d31704b1",
   "metadata": {},
   "source": [
    "## Encoding Decoding\n",
    "Encoding means assigning each word a unique ID. decoding means getting back the word from the integer ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3c27e75-e47a-4f7b-9fa5-910cd98dd74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {integer: string for string,integer in vocab.items()}\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "       \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1',text)\n",
    "        return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d634e81b-4a90-46b6-83fd-e1ba530f7a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 86, 2, 969, 1114, 694, 618, 853, 6, 1262, 687, 6, 1, 102, 8, 66, 970]\n",
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said\n"
     ]
    }
   ],
   "source": [
    "text=\"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said\"\"\"\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "ids=tokenizer.encode(text)\n",
    "regenerated_text = tokenizer.decode(ids)\n",
    "print(ids)\n",
    "print(regenerated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cdc9c6-a821-4e88-bc89-ed1259332cdc",
   "metadata": {},
   "source": [
    "## Adding  special tokens: `<unk>` and `<|endoftext|>` \n",
    "We add special tokens to a vocabulary to deal with certain contexts. For instance, we\n",
    "add an `<|unk|>` token to represent new and unknown words that were not part of the training\n",
    "data and thus not part of the existing vocabulary.\n",
    "<br>\n",
    "When working with multiple independent text source, we add `<|endoftext|>` tokens\n",
    "between these texts. These <|endoftext|> tokens act as markers, signaling the start or end of a particular segment, allowing for more effective processing and understanding by the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea423615-cc33-401b-88c8-82881fc4e0e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1267"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding `<|endoftext|>` token \n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend(['<|endoftext|>','<unk>'])\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "vocab['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eccf3e40-52e1-4220-b149-24df732b9094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improving tokenizer class\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {integer: string for string,integer in vocab.items()}\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int else '<unk>' for item in preprocessed]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "       \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1',text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a29b51e-9729-4a38-8c21-24e128ecd42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1267, 6, 431, 1262, 724, 1099, 18, 1266, 84, 1114, 1080, 1110, 826, 1114, 1267, 8]\n",
      "<unk>, do you like tea? <|endoftext|> In the sunlit terraces of the <unk>.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join([text1,text2])\n",
    "tokenizer =SimpleTokenizerV2(vocab)\n",
    "ids=  tokenizer.encode(text)\n",
    "regen_text = tokenizer.decode(ids)\n",
    "print(ids)\n",
    "print(regen_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e08040-7919-41e4-b4ba-0653d13f28d9",
   "metadata": {},
   "source": [
    "## Binary Pair Encoding\n",
    "Using `tiktoken`\n",
    "<br> BPE starts with adding all individual single characters to its vocabulary (\"a\",\n",
    "\"b\", ...). In the next stage, it merges character combinations that frequently\n",
    "occur together into subwords. For example, \"d\" and \"e\" may be merged into the subword \"de,\" which is common in many English words like \"define\",\n",
    "\"depend\", \"made\", and \"hidden\". The merges are determined by a frequency\n",
    "cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b844d64-848b-46bb-94d6-485f2e0f81fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3cb3bd6f-f7a9-4017-ac69-86935b4bfb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n",
      "['Ak', 'w', 'ir', 'w', ' ', 'ier']\n",
      "\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "text = \"Akwirw ier\"\n",
    "ids = tokenizer.encode(text,)\n",
    "tokens=[tokenizer.decode([id]) for id in ids]\n",
    "string = tokenizer.decode(ids)\n",
    "print(ids)\n",
    "print(tokens)\n",
    "print()\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "712573dc-6af3-4ac8-a349-032bb2b66b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33901"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ca7b2f-8058-428b-ac42-a1a632a9ce38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
