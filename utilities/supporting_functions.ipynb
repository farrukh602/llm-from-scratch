{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d981ddb9-386f-4d61-adce-0139f2c30654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ef451de-f429-4ec6-aff2-689da8531afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_config_124M = {\n",
    "    'vocab_size': 50257,\n",
    "    'context_length': 1024,\n",
    "    'emb_dim': 768, \n",
    "    'n_heads': 12,\n",
    "    'n_layers': 12,\n",
    "    'drop_rate': 0.1,\n",
    "    'qkv_bias': False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71098e94-005f-4751-b969-a6cbdb8631ca",
   "metadata": {},
   "source": [
    "- \"context_length\" denotes the maximum number of input tokens the\n",
    "model can handle, via the positional embeddings discussed in chapter 2.\n",
    "- \"emb_dim\" represents the embedding size, transforming each token into\n",
    "a 768-dimensional vector.\n",
    "- \"n_heads\" indicates the count of attention heads in the multi-head\n",
    "attention mechanism, as implemented in chapter 3.\n",
    "- \"n_layers\" specifies the number of transformer blocks in the model,\n",
    "which will be elaborated on in upcoming sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "2372b461-2e8b-46ab-a2ed-a9f39ddc7864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Head Attention Class\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in, d_out, context_length, dropout,num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out%num_heads==0, 'd_out must be divisible by num_heads'\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads #A\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out) #B\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "        'mask',\n",
    "        torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x) #C\n",
    "        queries = self.W_query(x) #C\n",
    "        values = self.W_value(x) #C\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) #D\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim) #D\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)#D\n",
    "        keys = keys.transpose(1, 2) #E\n",
    "        queries = queries.transpose(1, 2) #E\n",
    "        values = values.transpose(1, 2) #E\n",
    "        attn_scores = queries @ keys.transpose(2, 3) #F\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens] #G\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf) #H\n",
    "        attn_weights = torch.softmax(\n",
    "        attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) #I\n",
    "        #J\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) #K\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cc30dc-6d96-4f93-9610-93b7fa29e3f9",
   "metadata": {},
   "source": [
    "- The **FeedForward** module we implemented in this section plays a crucial role\n",
    "in enhancing the model's ability to learn from and generalize the data.\n",
    "Although the input and output dimensions of this module are the same, it\n",
    "internally expands the embedding dimension into a higher-dimensional space\n",
    "through the first linear layer. This expansion is\n",
    "followed by a non-linear GELU activation, and then a contraction back to the\n",
    "original dimension with the second linear transformation. Such a design\n",
    "allows for the exploration of a richer representation space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "84861c46-c62b-4a5f-938b-28df07e1f52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps=1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True)\n",
    "        norm_x = (x-mean)/torch.sqrt(var+self.eps)\n",
    "        return self.scale*norm_x+self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return 0.5*x*(1+torch.tanh(torch.sqrt(torch.tensor(2.0/torch.pi))*(x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg['emb_dim'], 4*cfg['emb_dim']),\n",
    "            GELU(),\n",
    "            nn.Linear(4*cfg['emb_dim'], cfg['emb_dim'])\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "# Transformer Block\n",
    "# input - LN1 - MHA - Drpout - (+input) - \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        vocab_size, context_length, emb_dim, num_heads, n_layers, drop_rate, qkv_bias = cfg.values()\n",
    "        embedding_layer = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.layernorm1 = LayerNorm(emb_dim)\n",
    "        self.layernorm2 = LayerNorm(emb_dim)\n",
    "        self.mha = MultiHeadAttention(d_in=emb_dim, d_out=emb_dim, context_length=context_length, dropout=drop_rate,\n",
    "                           num_heads=num_heads, qkv_bias=qkv_bias )\n",
    "        self.dropout = nn.Dropout(p=drop_rate)\n",
    "        self.ffn = FeedForward(cfg)\n",
    "    def forward(self, x):\n",
    "        #A\n",
    "        skip_connection=x\n",
    "        x = self.layernorm1(x) # pre normalization (before multihead atention and feed forward)\n",
    "        x = self.mha(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + skip_connection\n",
    "\n",
    "        #B\n",
    "        skip_connection=x\n",
    "        x = self.layernorm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x+skip_connection\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8a3c20-9f15-4710-bab8-043ee7060f74",
   "metadata": {},
   "source": [
    "### Final GPT2 architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06a9e7d-d6d3-41d5-bf36-47e3bb5e27ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51801e8-a13e-43ee-8a80-800b9c2c4bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "x = tok_embeds + pos_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be25d514-1eef-470b-bbe7-6eb10b6b8616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e49f75-9beb-4c31-9348-8861ec76c27e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "c7a1e2ba-7659-4d36-a659-a1ec4310ca37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 768]), torch.Size([2, 4, 768]))"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gpt_config_124M = {\n",
    "    'vocab_size': 50257,\n",
    "    'context_length': 1024,\n",
    "    'emb_dim': 768, \n",
    "    'n_heads': 12,\n",
    "    'n_layers': 12,\n",
    "    'drop_rate': 0.1,\n",
    "    'qkv_bias': False\n",
    "}\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "x = torch.rand(2,4,768)\n",
    "trf_block= TransformerBlock(gpt_config_124M)\n",
    "output = trf_block(x)\n",
    "\n",
    "x.shape, output.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "15e26e17-f13f-4c4f-9d4d-c4d0f078ecd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    " vocab_size, context_length, emb_dim, n_heads, n_layers, drop_rate, qkv_bias = gpt_config_124M.values()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "token_batch = torch.stack(batch,dim=0)\n",
    "print(token_batch)\n",
    "\n",
    "tok_embedding_layer = nn.Embedding(vocab_size, emb_dim)\n",
    "pos_embedding_layer = nn.Embedding(context_length, emb_dim)\n",
    "# embedding_batch =tok_embedding_layer(token_batch) + pos_embedding_layer(torch.arange(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "36c823a0-7336-4d01-80d1-f452a13d0695",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't use starred expression here (3829730376.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[238], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    *[\"Alice\", 30, \"USA\"]\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m can't use starred expression here\n"
     ]
    }
   ],
   "source": [
    "*[\"Alice\", 30, \"USA\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "2e759e2b-b957-4a5e-9c2f-99ac268d54a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Embedding(50257, 768), Embedding(1024, 768))"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_embedding_layer, pos_embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb2a44e-b7c0-428f-b9a4-825699a4e28e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
