{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "uFsddU0tcVcm",
        "outputId": "ecf6e23c-e661-4ce7-af36-6bfaf38c9a23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "uFsddU0tcVcm",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.1 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a707413c-c97e-472c-891b-f2f0538cbe51",
      "metadata": {
        "id": "a707413c-c97e-472c-891b-f2f0538cbe51"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tiktoken\n",
        "\n",
        "from utilities.gpt_module import GPTModel\n",
        "import importlib\n",
        "from utilities import supporting_modules\n",
        "from utilities.supporting_modules import create_dataloader, GPTDataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "49103f4a-bdfc-467b-b2f5-2de99c50b32d",
      "metadata": {
        "scrolled": true,
        "id": "49103f4a-bdfc-467b-b2f5-2de99c50b32d"
      },
      "outputs": [],
      "source": [
        "gpt_config_124m = {\n",
        "\"vocab_size\": 50257,\n",
        "\"context_length\": 256, #A\n",
        "\"emb_dim\": 768,\n",
        "\"n_heads\": 12,\n",
        "\"n_layers\": 12,\n",
        "\"drop_rate\": 0.1, #B\n",
        "\"qkv_bias\": False\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86118518-8941-4b09-94a7-06a2d23d6bb0",
      "metadata": {
        "id": "86118518-8941-4b09-94a7-06a2d23d6bb0"
      },
      "source": [
        "##### Text generation Helping functions\n",
        "A three-step text generation process using a GPT model:\n",
        "- First, the tokenizer converts input text into a series of token IDs,\n",
        "- Second, the model receives these token IDs and generates corresponding logits, which are vectors representing the probability distribution for each token in the vocabulary.\n",
        "- Third, these logits are converted back into token IDs, which the tokenizer decodes into human-readable text, completing the cycle from textual input to textual output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e457d819-b469-4e4f-8b44-d38b12e20017",
      "metadata": {
        "id": "e457d819-b469-4e4f-8b44-d38b12e20017"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate_text(model, idx, max_new_tokens, context_size): #A\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :] #C\n",
        "        probas = torch.softmax(logits, dim=-1) #D\n",
        "        idx_next = torch.argmax(probas, dim=-1, keepdim=True) #E\n",
        "        idx = torch.cat((idx, idx_next), dim=1) #F\n",
        "    return idx\n",
        "\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # to add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # to remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe2fdd85-7611-416d-83c0-2910e2c02b42",
      "metadata": {
        "id": "fe2fdd85-7611-416d-83c0-2910e2c02b42"
      },
      "source": [
        "### Setting the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a405e3cd-1a1f-41f4-91f2-5efb53c13347",
      "metadata": {
        "id": "a405e3cd-1a1f-41f4-91f2-5efb53c13347"
      },
      "outputs": [],
      "source": [
        "file_path = \"data/the_verdict.txt\"\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    text_data = file.read()\n",
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]\n",
        "\n",
        "# crafting data loaders\n",
        "train_loader = create_dataloader(\n",
        "train_data,\n",
        "batch_size=2,\n",
        "max_length=gpt_config_124m[\"context_length\"],\n",
        "stride=gpt_config_124m[\"context_length\"],\n",
        "drop_last=True,\n",
        "shuffle=True\n",
        ")\n",
        "val_loader = create_dataloader(\n",
        "val_data,\n",
        "batch_size=2,\n",
        "max_length=gpt_config_124m[\"context_length\"],\n",
        "stride=gpt_config_124m[\"context_length\"],\n",
        "drop_last=False,\n",
        "shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b852583e-8c7a-41ad-bde1-ef49dd512e23",
      "metadata": {
        "id": "b852583e-8c7a-41ad-bde1-ef49dd512e23"
      },
      "source": [
        "### Training Function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_batch_loss(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "def loss_calculator(data_loader, model, loss_function, device, num_batches=9):\n",
        "    total_loss = 0.\n",
        "    if num_batches is None:\n",
        "        num_batches = len(data_loader) # A\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(data_loader)) # B\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = loss_function(input_batch, target_batch, model, device)\n",
        "            total_loss += loss\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches"
      ],
      "metadata": {
        "id": "O_wJDKZvfiLF"
      },
      "id": "O_wJDKZvfiLF",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "678b6a6d-d3de-4619-8e2b-848ad9ff0103",
      "metadata": {
        "id": "678b6a6d-d3de-4619-8e2b-848ad9ff0103"
      },
      "outputs": [],
      "source": [
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text(model=model, idx=encoded,\n",
        "        max_new_tokens=50, context_size=context_size)\n",
        "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "        print(decoded_text.replace(\"\\n\", \" \")) # Compact print format\n",
        "    model.train()\n",
        "\n",
        "\n",
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval() #A\n",
        "    with torch.no_grad(): #B\n",
        "        train_loss = loss_calculator(train_loader, model, cross_entropy_batch_loss, device, num_batches=eval_iter)\n",
        "        val_loss = loss_calculator(val_loader, model,  cross_entropy_batch_loss, device,num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "358b3e2c-de95-4260-927c-426a7521e0d0",
      "metadata": {
        "id": "358b3e2c-de95-4260-927c-426a7521e0d0"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer, device,\n",
        "               eval_freq, eval_iter, num_epochs, start_context):\n",
        "    train_losses, val_losses, track_token_seens = [],[], []\n",
        "    tokens_seen, global_step = 0,-1\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            loss = loss_calculator(train_loader, model, cross_entropy_batch_loss, device)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            tokens_seen +=input_batch.numel()\n",
        "            global_step+=1\n",
        "            if global_step%eval_freq==0:\n",
        "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_token_seens.append(tokens_seen)\n",
        "                print(f\"Epoch: {epoch+1} (Step: {global_step: 06d}):\",\n",
        "                     f\"Train loss: {train_loss: .3f}, Val loss: {val_loss: .3f}\")\n",
        "            generate_and_print_sample( model, train_loader.dataset.tokenizer, device, start_context)\n",
        "    return train_losses, val_losses, track_token_seens\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GPTModel(gpt_config_124m)\n",
        "\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0)\n",
        "\n",
        "\n",
        "num_epochs = 10\n",
        "train_losses, val_losses, tokens_seen = train_model(model, train_loader, val_loader,\n",
        "                                                    optimizer, device,\n",
        "                                                    eval_freq=5, eval_iter=1,\n",
        "                                                    num_epochs=num_epochs,\n",
        "                                                    start_context=\"Every effort moves you\")"
      ],
      "metadata": {
        "id": "1SMK7CKYdBww",
        "outputId": "59593e05-b9a9-4578-b2bc-ee743d30af11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "id": "1SMK7CKYdBww",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 148.00 MiB. GPU ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-9418c693b0fb>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPTModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpt_config_124m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0004\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1157\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m                     )\n\u001b[0;32m-> 1159\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1160\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 148.00 MiB. GPU "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ggG2iz-UdBuc"
      },
      "id": "ggG2iz-UdBuc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bbNXFtMqdBr9"
      },
      "id": "bbNXFtMqdBr9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3BFxgdrCdBpl"
      },
      "id": "3BFxgdrCdBpl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "41YLcTBvdBm6"
      },
      "id": "41YLcTBvdBm6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "49vYSVNFdBkV"
      },
      "id": "49vYSVNFdBkV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-RhN8G65dBhy"
      },
      "id": "-RhN8G65dBhy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yZYtVBoTdBfG"
      },
      "id": "yZYtVBoTdBfG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3e15df4-6b60-4dab-a856-6d0189e5824e",
      "metadata": {
        "id": "b3e15df4-6b60-4dab-a856-6d0189e5824e"
      },
      "outputs": [],
      "source": [
        "device='cpu'\n",
        "model.to(device)\n",
        "loss = loss_calculator(train_loader, model, cross_entropy_batch_loss, device)\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6158d166-017e-4232-9160-af382444540d",
      "metadata": {
        "id": "6158d166-017e-4232-9160-af382444540d"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_batch_loss(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "def loss_calculator(data_loader, model, loss_function, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if num_batches is None:\n",
        "        num_batches = len(data_loader) # A\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(data_loader)) # B\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = loss_function(input_batch, target_batch, model, device)\n",
        "            total_loss += loss\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd00e1f0-c00e-4de4-9d95-05a2684e5f4a",
      "metadata": {
        "id": "dd00e1f0-c00e-4de4-9d95-05a2684e5f4a",
        "outputId": "e808ceee-7700-4a05-ae92-a16db1b186ee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 256])"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a=[2,4,5]\n",
        "torch.tensor(a,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19bd1c5e-34ae-4cef-afb2-42df8bd6e89a",
      "metadata": {
        "id": "19bd1c5e-34ae-4cef-afb2-42df8bd6e89a",
        "outputId": "d1747378-4572-4fe9-9b90-0cb66a7acd97"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'float' object has no attribute 'backward'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[96], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m()\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'backward'"
          ]
        }
      ],
      "source": [
        "loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4ea28c3-cbb5-4f1d-82ad-fee27fb32fb8",
      "metadata": {
        "id": "e4ea28c3-cbb5-4f1d-82ad-fee27fb32fb8"
      },
      "outputs": [],
      "source": [
        "criterion=nn.CrossEntropyLoss()\n",
        "def loss_calculator2(train_loader, model, criterion, device):\n",
        "    # Dummy implementation for the sake of example\n",
        "    # model.train()\n",
        "    for data, target_batch in train_loader:\n",
        "        data, target_batch = data.to(device), target_batch.to(device)\n",
        "        # optimizer.zero_grad()\n",
        "        logits = model(data)\n",
        "        loss = criterion(logits.flatten(0, 1), target_batch.flatten())\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35332cbb-32e9-4efc-8243-e1513b66a012",
      "metadata": {
        "id": "35332cbb-32e9-4efc-8243-e1513b66a012"
      },
      "outputs": [],
      "source": [
        "loss2.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15c10bf0-6632-4107-8bde-9521ccf1abcb",
      "metadata": {
        "id": "15c10bf0-6632-4107-8bde-9521ccf1abcb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}